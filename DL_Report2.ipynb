{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMXSanmeiA5gb2diWhlUPxa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kwannn666/DL_Report2/blob/main/DL_Report2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OzGnZ8dRm7BQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5659187a-71b6-476f-eee0-dc3b2d59729a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.5/962.5 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h--2025-06-10 08:48:10--  https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.107.86, 52.217.140.80, 54.231.133.88, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.107.86|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99003388 (94M) [application/x-tar]\n",
            "Saving to: ‘imagenette2-160.tgz’\n",
            "\n",
            "imagenette2-160.tgz 100%[===================>]  94.42M  29.0MB/s    in 3.3s    \n",
            "\n",
            "2025-06-10 08:48:14 (29.0 MB/s) - ‘imagenette2-160.tgz’ saved [99003388/99003388]\n",
            "\n",
            "--2025-06-10 08:48:16--  http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
            "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
            "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1999639040 (1.9G) [application/x-tar]\n",
            "Saving to: ‘VOCtrainval_11-May-2012.tar’\n",
            "\n",
            "VOCtrainval_11-May- 100%[===================>]   1.86G  54.2MB/s    in 29s     \n",
            "\n",
            "2025-06-10 08:48:45 (65.6 MB/s) - ‘VOCtrainval_11-May-2012.tar’ saved [1999639040/1999639040]\n",
            "\n",
            "--2025-06-10 08:48:59--  http://images.cocodataset.org/zips/val2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.165.41, 52.217.225.65, 16.15.193.72, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.165.41|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815585330 (778M) [application/zip]\n",
            "Saving to: ‘val2017.zip’\n",
            "\n",
            "val2017.zip         100%[===================>] 777.80M  37.8MB/s    in 21s     \n",
            "\n",
            "2025-06-10 08:49:20 (37.8 MB/s) - ‘val2017.zip’ saved [815585330/815585330]\n",
            "\n",
            "--2025-06-10 08:49:29--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.160.97, 52.216.44.169, 52.217.234.161, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.160.97|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘annotations_trainval2017.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  41.8MB/s    in 8.4s    \n",
            "\n",
            "2025-06-10 08:49:37 (28.9 MB/s) - ‘annotations_trainval2017.zip’ saved [252907541/252907541]\n",
            "\n",
            "mv: cannot move 'data/tmp/annotations/instances_val2017.json' to 'data/mini_coco_det/annotations/': Not a directory\n"
          ]
        }
      ],
      "source": [
        "# 📦 安裝套件與清理舊資料\n",
        "!pip install -q torchvision lightning\n",
        "\n",
        "# ✅ 清除殘留資料（避免解壓時交互式提示）\n",
        "!rm -rf data\n",
        "!rm -rf imagenette2-160.tgz VOCtrainval_11-May-2012.tar val2017.zip annotations_trainval2017.zip\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import json\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 固定亂數種子\n",
        "random.seed(42)\n",
        "\n",
        "# ✅ 建立主結構\n",
        "base_dir = \"data\"\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "for name in [\"mini_coco_det\", \"mini_voc_seg\", \"imagenette_160\"]:\n",
        "    for sub in [\"train\", \"val\"]:\n",
        "        os.makedirs(os.path.join(base_dir, name, sub), exist_ok=True)\n",
        "\n",
        "# ✅ 下載資料\n",
        "!wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\n",
        "!tar -xf imagenette2-160.tgz -C data/\n",
        "!mv data/imagenette2-160 data/imagenette_160_raw\n",
        "\n",
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
        "!tar -xf VOCtrainval_11-May-2012.tar -C data/\n",
        "\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip\n",
        "!unzip -q val2017.zip -d data/mini_coco_det/train\n",
        "\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "!unzip -q annotations_trainval2017.zip -d data/tmp\n",
        "!mv data/tmp/annotations/instances_val2017.json data/mini_coco_det/annotations/\n",
        "!rm -r data/tmp\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 建立正確的目錄（如果還沒建立）\n",
        "!mkdir -p data/mini_coco_det/annotations\n",
        "\n",
        "# 將標註檔搬到正式目錄中\n",
        "!mv data/tmp/annotations/instances_val2017.json data/mini_coco_det/annotations/\n",
        "\n",
        "# 刪除暫存資料夾\n",
        "!rm -r data/tmp\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQxTrD0d5wz4",
        "outputId": "b39c24f2-eeac-4b4f-8c73-1670af9388c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'data/tmp/annotations/instances_val2017.json': No such file or directory\n",
            "rm: cannot remove 'data/tmp': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 🔄 清除舊的目標資料夾（不刪原始解壓檔）\n",
        "# =============================\n",
        "for subdir in [\"data/imagenette_160\", \"data/mini_voc_seg\", \"data/mini_coco_det\"]:\n",
        "    if os.path.exists(subdir):\n",
        "        shutil.rmtree(subdir)\n",
        "    os.makedirs(subdir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "wJP3k24PGrum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q annotations_trainval2017.zip -d data/tmp\n"
      ],
      "metadata": {
        "id": "Uh3HiSF9Dlus"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data/tmp/annotations/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUWQX4OnDsL0",
        "outputId": "0d4d7b4e-c0a0-40b5-bc6b-517fb5074c8b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "captions_train2017.json   instances_val2017.json\n",
            "captions_val2017.json\t  person_keypoints_train2017.json\n",
            "instances_train2017.json  person_keypoints_val2017.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data/mini_coco_det/annotations\n",
        "!mv data/tmp/annotations/instances_val2017.json data/mini_coco_det/annotations/\n",
        "!rm -r data/tmp\n"
      ],
      "metadata": {
        "id": "uj_0HzAsDvwS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.path.exists(\"data/mini_coco_det/annotations/instances_val2017.json\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Wje9S-WDgJe",
        "outputId": "86c51f16-8450-495c-8e67-3b3cd677d08f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, random, json, glob\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "\n",
        "# =============================\n",
        "# 1️⃣ Imagenette 抽樣與複製\n",
        "# =============================\n",
        "imagenette_src = Path(\"data/imagenette_160_raw/train\")\n",
        "imagenette_dst_train = Path(\"data/imagenette_160/train\")\n",
        "imagenette_dst_val = Path(\"data/imagenette_160/val\")\n",
        "imagenette_dst_train.mkdir(parents=True, exist_ok=True)\n",
        "imagenette_dst_val.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "all_imgs = list(imagenette_src.rglob(\"*.JPEG\"))\n",
        "print(f\"📦 找到 Imagenette 原始圖片數量：{len(all_imgs)}\")\n",
        "if len(all_imgs) < 300:\n",
        "    raise ValueError(\"❌ Imagenette 圖片數不足 300 張，請確認是否正確解壓 imagenette2-160.tgz\")\n",
        "\n",
        "selected = random.sample(all_imgs, 300)\n",
        "train_imgs, val_imgs = selected[:240], selected[240:]\n",
        "\n",
        "for img in tqdm(train_imgs, desc=\"Imagenette Train\"):\n",
        "    shutil.copy(img, imagenette_dst_train / img.name)\n",
        "for img in tqdm(val_imgs, desc=\"Imagenette Val\"):\n",
        "    shutil.copy(img, imagenette_dst_val / img.name)\n",
        "\n",
        "# =============================\n",
        "# 2️⃣ VOC Segmentation 抽樣與遮罩對應\n",
        "# =============================\n",
        "voc_img_dir = Path(\"data/VOCdevkit/VOC2012/JPEGImages\")\n",
        "voc_mask_dir = Path(\"data/VOCdevkit/VOC2012/SegmentationClass\")\n",
        "voc_dst_train = Path(\"data/mini_voc_seg/train\")\n",
        "voc_dst_val = Path(\"data/mini_voc_seg/val\")\n",
        "voc_dst_train.mkdir(parents=True, exist_ok=True)\n",
        "voc_dst_val.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "all_voc_images = list(voc_img_dir.glob(\"*.jpg\"))\n",
        "mask_names = {p.stem for p in voc_mask_dir.glob(\"*.png\")}\n",
        "valid_voc_images = [img for img in all_voc_images if img.stem in mask_names]\n",
        "\n",
        "print(f\"📦 可用 VOC 圖片（含遮罩）數量：{len(valid_voc_images)}\")\n",
        "if len(valid_voc_images) < 300:\n",
        "    raise ValueError(\"❌ VOC 可用圖片數不足 300 張，請確認 VOCtrainval 是否正確解壓\")\n",
        "\n",
        "selected_voc = random.sample(valid_voc_images, 300)\n",
        "voc_train = selected_voc[:240]\n",
        "voc_val = selected_voc[240:]\n",
        "\n",
        "def copy_voc(img_list, target_dir):\n",
        "    for img in tqdm(img_list, desc=f\"VOC → {target_dir.name}\"):\n",
        "        mask = voc_mask_dir / (img.stem + \".png\")\n",
        "        if mask.exists():\n",
        "            shutil.copy(img, target_dir / img.name)\n",
        "            shutil.copy(mask, target_dir / mask.name)\n",
        "\n",
        "copy_voc(voc_train, voc_dst_train)\n",
        "copy_voc(voc_val, voc_dst_val)\n",
        "\n",
        "# =============================\n",
        "# 3️⃣ COCO Detection 抽樣與標註過濾\n",
        "# =============================\n",
        "coco_img_dir = Path(\"data/mini_coco_det/train/val2017\")\n",
        "coco_dst_train = Path(\"data/mini_coco_det/train\")\n",
        "coco_dst_val = Path(\"data/mini_coco_det/val\")\n",
        "coco_dst_train.mkdir(parents=True, exist_ok=True)\n",
        "coco_dst_val.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "all_coco_images = list(coco_img_dir.glob(\"*.jpg\"))\n",
        "print(f\"📦 找到 COCO val2017 圖片數量：{len(all_coco_images)}\")\n",
        "if len(all_coco_images) < 300:\n",
        "    raise ValueError(\"❌ COCO 圖片不足 300 張，請確認 val2017 是否正確解壓\")\n",
        "\n",
        "selected_coco = random.sample(all_coco_images, 300)\n",
        "coco_train, coco_val = selected_coco[:240], selected_coco[240:]\n",
        "\n",
        "for img in tqdm(coco_train, desc=\"COCO Train\"):\n",
        "    shutil.copy(img, coco_dst_train / img.name)\n",
        "for img in tqdm(coco_val, desc=\"COCO Val\"):\n",
        "    shutil.copy(img, coco_dst_val / img.name)\n",
        "\n",
        "# 指定 COCO 標註檔\n",
        "coco_ann_path = Path(\"data/mini_coco_det/annotations/instances_val2017.json\")\n",
        "if not coco_ann_path.exists():\n",
        "    raise FileNotFoundError(\"❌ 找不到 COCO 標註檔 instances_val2017.json，請確認是否正確搬移\")\n",
        "\n",
        "with open(coco_ann_path) as f:\n",
        "    coco_ann = json.load(f)\n",
        "\n",
        "valid_img_names = {p.name for p in selected_coco}\n",
        "valid_img_ids = set()\n",
        "filtered_images = []\n",
        "for img in coco_ann[\"images\"]:\n",
        "    if img[\"file_name\"] in valid_img_names:\n",
        "        filtered_images.append(img)\n",
        "        valid_img_ids.add(img[\"id\"])\n",
        "\n",
        "filtered_anns = [ann for ann in coco_ann[\"annotations\"] if ann[\"image_id\"] in valid_img_ids]\n",
        "filtered_coco = {\n",
        "    \"info\": coco_ann.get(\"info\", {}),\n",
        "    \"licenses\": coco_ann.get(\"licenses\", []),\n",
        "    \"categories\": coco_ann[\"categories\"],\n",
        "    \"images\": filtered_images,\n",
        "    \"annotations\": filtered_anns\n",
        "}\n",
        "\n",
        "# 儲存過濾後的標註 JSON\n",
        "mini_json_path = Path(\"data/mini_coco_det/annotations/instances_val2017.json\")\n",
        "with open(mini_json_path, \"w\") as f:\n",
        "    json.dump(filtered_coco, f)\n",
        "\n",
        "# 移除原始 val2017\n",
        "shutil.rmtree(coco_img_dir)\n",
        "\n",
        "print(\"\\n✅ 所有 Mini Dataset 準備完成！三組資料各包含 train 240 張、val 60 張。\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqNQUhUu5hnr",
        "outputId": "ed1cac7f-c375-4e13-c764-6e2cc120891a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 找到 Imagenette 原始圖片數量：9469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Imagenette Train: 100%|██████████| 240/240 [00:00<00:00, 6056.25it/s]\n",
            "Imagenette Val: 100%|██████████| 60/60 [00:00<00:00, 5459.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 可用 VOC 圖片（含遮罩）數量：2913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "VOC → train: 100%|██████████| 240/240 [00:00<00:00, 954.48it/s]\n",
            "VOC → val: 100%|██████████| 60/60 [00:00<00:00, 889.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 找到 COCO val2017 圖片數量：5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "COCO Train: 100%|██████████| 240/240 [00:00<00:00, 2719.51it/s]\n",
            "COCO Val: 100%|██████████| 60/60 [00:00<00:00, 2454.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 所有 Mini Dataset 準備完成！三組資料各包含 train 240 張、val 60 張。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔄 清空原有 VOC 輸出資料夾\n",
        "!rm -rf data/mini_voc_seg/train/*\n",
        "!rm -rf data/mini_voc_seg/val/*\n",
        "\n",
        "# 🟩 重建 VOC train/val\n",
        "voc_img_dir = Path(\"data/VOCdevkit/VOC2012/JPEGImages\")\n",
        "voc_mask_dir = Path(\"data/VOCdevkit/VOC2012/SegmentationClass\")\n",
        "voc_dst_train = Path(\"data/mini_voc_seg/train\")\n",
        "voc_dst_val = Path(\"data/mini_voc_seg/val\")\n",
        "voc_dst_train.mkdir(parents=True, exist_ok=True)\n",
        "voc_dst_val.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "all_voc_images = list(voc_img_dir.glob(\"*.jpg\"))\n",
        "mask_names = {p.stem for p in voc_mask_dir.glob(\"*.png\")}\n",
        "valid_voc_images = [img for img in all_voc_images if img.stem in mask_names]\n",
        "\n",
        "print(f\"📦 可用 VOC 圖片（含遮罩）數量：{len(valid_voc_images)}\")\n",
        "selected_voc = random.sample(valid_voc_images, 150)\n",
        "voc_train = selected_voc[:120]\n",
        "voc_val = selected_voc[120:]\n",
        "\n",
        "def copy_voc(img_list, target_dir):\n",
        "    for img in tqdm(img_list, desc=f\"VOC → {target_dir.name}\"):\n",
        "        mask = voc_mask_dir / (img.stem + \".png\")\n",
        "        if mask.exists():\n",
        "            shutil.copy(img, target_dir / img.name)\n",
        "            shutil.copy(mask, target_dir / mask.name)\n",
        "\n",
        "copy_voc(voc_train, voc_dst_train)\n",
        "copy_voc(voc_val, voc_dst_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9C1K_3xIF2h",
        "outputId": "958ea99e-f777-432c-9d38-f78d007df0c7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 可用 VOC 圖片（含遮罩）數量：2913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "VOC → train: 100%|██████████| 120/120 [00:00<00:00, 1407.40it/s]\n",
            "VOC → val: 100%|██████████| 30/30 [00:00<00:00, 588.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os\n",
        "\n",
        "# 🔄 刪除整個 train/val 資料夾\n",
        "shutil.rmtree(\"data/mini_voc_seg/train\", ignore_errors=True)\n",
        "shutil.rmtree(\"data/mini_voc_seg/val\", ignore_errors=True)\n",
        "\n",
        "# ✅ 重新建立資料夾\n",
        "os.makedirs(\"data/mini_voc_seg/train\", exist_ok=True)\n",
        "os.makedirs(\"data/mini_voc_seg/val\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "eQzA4EL5Iu4U"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def count_images(folder):\n",
        "    return len(list(Path(folder).glob(\"*.jpg\"))) + len(list(Path(folder).glob(\"*.jpeg\"))) + len(list(Path(folder).glob(\"*.png\")))+ len(list(Path(folder).glob(\"*.JPEG\")))\n",
        "\n",
        "print(\"COCO Train:\", count_images(\"data/mini_coco_det/train\"))\n",
        "print(\"COCO Val:\", count_images(\"data/mini_coco_det/val\"))\n",
        "\n",
        "print(\"VOC Train:\", count_images(\"data/mini_voc_seg/train\"))\n",
        "print(\"VOC Val:\", count_images(\"data/mini_voc_seg/val\"))\n",
        "\n",
        "print(\"Imagenette Train:\", count_images(\"data/imagenette_160/train\"))\n",
        "print(\"Imagenette Val:\", count_images(\"data/imagenette_160/val\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KoQQbag7mVl",
        "outputId": "ae5b1bbd-ccf4-4914-e6b0-53effeb309ec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COCO Train: 240\n",
            "COCO Val: 60\n",
            "VOC Train: 240\n",
            "VOC Val: 60\n",
            "Imagenette Train: 240\n",
            "Imagenette Val: 60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "i6BHnumt8xJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#顯示三個任務資料夾的總檔案大小\n",
        "def get_folder_size_mb(path):\n",
        "    total_size = 0\n",
        "    for dirpath, _, filenames in os.walk(path):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            if os.path.isfile(fp):\n",
        "                total_size += os.path.getsize(fp)\n",
        "    return round(total_size / (1024 * 1024), 2)  # 轉成 MB 並四捨五入\n",
        "\n",
        "# 三個資料夾路徑\n",
        "folders = {\n",
        "    \"mini_coco_det\": \"data/mini_coco_det\",\n",
        "    \"mini_voc_seg\": \"data/mini_voc_seg\",\n",
        "    \"imagenette_160\": \"data/imagenette_160\"\n",
        "}\n",
        "\n",
        "# 印出大小\n",
        "print(\"📦 各資料夾總檔案大小（MB）\")\n",
        "for name, path in folders.items():\n",
        "    size_mb = get_folder_size_mb(path)\n",
        "    print(f\"{name.ljust(20)}: {size_mb} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "valr1UuX_eSJ",
        "outputId": "c23306fe-a246-460c-8a79-f8db36aa9996"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 各資料夾總檔案大小（MB）\n",
            "mini_coco_det       : 48.53 MB\n",
            "mini_voc_seg        : 17.08 MB\n",
            "imagenette_160      : 2.27 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q thop"
      ],
      "metadata": {
        "id": "UhH9CWvGMBK8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork, LastLevelMaxPool\n",
        "from thop import profile\n",
        "import time"
      ],
      "metadata": {
        "id": "ZNEOrVQEL2os"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧠 統一模型定義\n",
        "class UnifiedModel(nn.Module):\n",
        "    def __init__(self, num_classes_cls=10, num_classes_det=10, num_classes_seg=21):\n",
        "        super(UnifiedModel, self).__init__()\n",
        "\n",
        "        # 🔗 Backbone: MobileNetV3 Small\n",
        "        backbone = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
        "        self.backbone = backbone.features\n",
        "\n",
        "        # 🔗 FPN Neck：接 3 層輸出 (channel=24, 48, 96)\n",
        "        self.fpn = FeaturePyramidNetwork(\n",
        "           in_channels_list=[24, 40, 576],  # ← 修正這裡\n",
        "           out_channels=128,\n",
        "           extra_blocks=LastLevelMaxPool()\n",
        "        )\n",
        "\n",
        "\n",
        "        # 🧠 Shared conv head\n",
        "        self.shared_head = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # 🎯 Output heads\n",
        "        self.classifier = nn.Linear(128, num_classes_cls)         # 分類\n",
        "        self.det_head = nn.Conv2d(128, num_classes_det * 5, 1)    # 偵測 (class + bbox)\n",
        "        self.seg_head = nn.Conv2d(128, num_classes_seg, 1)        # 分割\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = []\n",
        "        for i, layer in enumerate(self.backbone):\n",
        "            x = layer(x)\n",
        "            if i in [3, 6, 12]:\n",
        "                feats.append(x)\n",
        "\n",
        "        fpn_feats = self.fpn({str(i): f for i, f in enumerate(feats)})\n",
        "        fpn_out = list(fpn_feats.values())[0]\n",
        "\n",
        "        shared = self.shared_head(fpn_out)\n",
        "        B, C, H, W = shared.shape\n",
        "\n",
        "        cls_logits = self.classifier(torch.mean(shared.view(B, C, -1), dim=2))\n",
        "        det_raw = self.det_head(shared)\n",
        "        det_output = det_raw.permute(0, 2, 3, 1).reshape(B, -1, 5)\n",
        "        seg_mask = self.seg_head(shared)\n",
        "\n",
        "        return cls_logits, det_output, seg_mask\n",
        "\n",
        "# 📦 參數數量統計\n",
        "def count_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# ⏱️ 推論時間測量（ms）\n",
        "def measure_inference_time(model, input_shape=(1, 3, 512, 512), device=\"cpu\"):\n",
        "    x = torch.randn(input_shape).to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(5):  # warm-up\n",
        "            _ = model(x)\n",
        "        start = time.time()\n",
        "        for _ in range(10):\n",
        "            _ = model(x)\n",
        "        end = time.time()\n",
        "    return (end - start) / 10 * 1000  # 單位: ms\n",
        "\n",
        "# 🚀 建立與測試\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = UnifiedModel().to(device)\n",
        "params = count_params(model)\n",
        "inference_ms = measure_inference_time(model, device=device)\n",
        "\n",
        "# ✅ 顯示結果\n",
        "print(f\"✅ 模型參數數量：{params:,} 個\")\n",
        "print(f\"✅ 單張推論時間：{inference_ms:.2f} ms\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "susIp5NfL7yo",
        "outputId": "c123e33b-0c31-4230-e3e3-08d58e6b5033"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n",
            "100%|██████████| 9.83M/9.83M [00:00<00:00, 78.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 模型參數數量：1,758,193 個\n",
            "✅ 單張推論時間：29.23 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "\n",
        "# ----------- Segmentation (VOC) Dataset -----------\n",
        "class VOCSegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None, target_transform=None):\n",
        "        self.image_paths = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg')])\n",
        "        self.mask_paths = [p.replace('.jpg', '.png') for p in self.image_paths]\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        mask = Image.open(self.mask_paths[idx]).convert('L')  # segmentation mask\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        if self.target_transform:\n",
        "            mask = self.target_transform(mask)\n",
        "        mask = mask.long()\n",
        "        # --- 防呆：只允許 0~20/255 ---\n",
        "        mask = mask.clone()\n",
        "        mask[(mask > 20) & (mask != 255)] = 255\n",
        "        return img, mask\n",
        "\n",
        "# ----------- Detection (COCO mini) Dataset -----------\n",
        "class MiniCocoDetection(Dataset):\n",
        "    def __init__(self, img_dir, ann_path, transform=None):\n",
        "        self.img_dir = Path(img_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "        with open(ann_path) as f:\n",
        "            coco = json.load(f)\n",
        "\n",
        "        self.img_id_to_filename = {img['id']: img['file_name'] for img in coco['images']}\n",
        "        self.ann_by_img = {img_id: [] for img_id in self.img_id_to_filename}\n",
        "        for ann in coco['annotations']:\n",
        "            self.ann_by_img[ann['image_id']].append(ann)\n",
        "\n",
        "        self.ids = list(self.img_id_to_filename.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.ids[idx]\n",
        "        file_name = self.img_id_to_filename[img_id]\n",
        "        img_path = self.img_dir / file_name\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        anns = self.ann_by_img[img_id]\n",
        "\n",
        "        boxes = []\n",
        "        for ann in anns:\n",
        "            x, y, w, h = ann['bbox']\n",
        "            boxes.append([x, y, x + w, y + h])\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        # 目標格式為 (N, 5): cx, cy, w, h, 1\n",
        "        targets = []\n",
        "        for box in boxes:\n",
        "            cx = (box[0] + box[2]) / 2\n",
        "            cy = (box[1] + box[3]) / 2\n",
        "            w = box[2] - box[0]\n",
        "            h = box[3] - box[1]\n",
        "            targets.append([cx, cy, w, h, 1])  # 1 for conf\n",
        "        targets = torch.tensor(targets, dtype=torch.float32)\n",
        "        return img, targets\n",
        "\n",
        "# ----------- Classification (Imagenette) Dataset -----------\n",
        "def build_cls_dataset(root_dir, input_size=512):\n",
        "    transform = transforms.Compose([transforms.Resize((input_size, input_size)), transforms.ToTensor()])\n",
        "    train_set = ImageFolder(root=Path(root_dir) / \"train\", transform=transform)\n",
        "    val_set = ImageFolder(root=Path(root_dir) / \"val\", transform=transform)\n",
        "    return train_set, val_set\n"
      ],
      "metadata": {
        "id": "E6kg-jRVaUEr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "det_train = MiniCocoDetection(\n",
        "    \"data/mini_coco_det/train\",\n",
        "    \"data/mini_coco_det/annotations/mini_instances_val2017.json\",\n",
        "    transform=det_tf\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "UPmGhkSAfiXw",
        "outputId": "d3a79be2-4768-475a-8764-3120dab6c566"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'det_tf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-366c8b71cf6f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"data/mini_coco_det/train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"data/mini_coco_det/annotations/mini_instances_val2017.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdet_tf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'det_tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "# mIoU 計算\n",
        "def compute_mIoU(preds, targets, num_classes=21):\n",
        "    preds = preds.cpu().numpy()\n",
        "    targets = targets.cpu().numpy()\n",
        "    ious = []\n",
        "    for cls in range(num_classes):\n",
        "        pred_inds = preds == cls\n",
        "        target_inds = targets == cls\n",
        "        intersection = (pred_inds & target_inds).sum()\n",
        "        union = (pred_inds | target_inds).sum()\n",
        "        if union == 0:\n",
        "            continue\n",
        "        ious.append(intersection / union)\n",
        "    return np.mean(ious) if ious else 0\n",
        "\n",
        "# Segmentation 訓練\n",
        "def train_segmentation_stage1(model, train_dir, val_dir, epochs=10, batch_size=8, lr=1e-3, device=\"cuda\"):\n",
        "    input_size = 512\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((input_size, input_size)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    mask_transform = transforms.Compose([\n",
        "        transforms.Resize((input_size, input_size), interpolation=Image.NEAREST),\n",
        "        transforms.PILToTensor()\n",
        "    ])\n",
        "\n",
        "    train_set = VOCSegmentationDataset(train_dir, transform, mask_transform)\n",
        "    val_set = VOCSegmentationDataset(val_dir, transform, mask_transform)\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=1)\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
        "    best_mIoU = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        for imgs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            imgs, masks = imgs.to(device), masks.to(device)\n",
        "            _, _, seg = model(imgs)\n",
        "            seg_H, seg_W = seg.shape[2:]\n",
        "\n",
        "            # === 強制修正 mask 的值範圍 ===\n",
        "            if masks.ndim == 3:\n",
        "                masks = masks.unsqueeze(1)\n",
        "            masks = masks.float()\n",
        "            masks = torch.nn.functional.interpolate(masks, size=(seg_H, seg_W), mode=\"nearest\")\n",
        "            masks = masks.squeeze(1).long()\n",
        "            masks = masks.clone()\n",
        "            masks[(masks > 20) & (masks != 255)] = 255\n",
        "\n",
        "            loss = criterion(seg, masks)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"🔧 Epoch {epoch+1} Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        all_preds, all_targets = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, masks in val_loader:\n",
        "                imgs, masks = imgs.to(device), masks.to(device)\n",
        "                _, _, seg = model(imgs)\n",
        "                seg_H, seg_W = seg.shape[2:]\n",
        "                if masks.ndim == 3:\n",
        "                    masks = masks.unsqueeze(1)\n",
        "                masks = masks.float()\n",
        "                masks = torch.nn.functional.interpolate(masks, size=(seg_H, seg_W), mode=\"nearest\")\n",
        "                masks = masks.squeeze(1).long()\n",
        "                masks = masks.clone()\n",
        "                masks[(masks > 20) & (masks != 255)] = 255\n",
        "\n",
        "                preds = torch.argmax(seg, dim=1)\n",
        "                all_preds.append(preds)\n",
        "                all_targets.append(masks)\n",
        "        all_preds = torch.cat(all_preds)\n",
        "        all_targets = torch.cat(all_targets)\n",
        "        mIoU = compute_mIoU(all_preds, all_targets)\n",
        "        best_mIoU = max(best_mIoU, mIoU)\n",
        "        print(f\"📊 mIoU: {mIoU:.4f}\")\n",
        "    print(f\"\\n✅ Stage 1 完成！記錄 mIoU_base = {best_mIoU:.4f}\")\n",
        "    return best_mIoU\n",
        "\n",
        "# mIoU for segmentation快速評估\n",
        "def evaluate_segmentation(model, val_loader, device=\"cuda\", num_samples=20):\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for i, (imgs, masks) in enumerate(val_loader):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "            imgs, masks = imgs.to(device), masks.to(device)\n",
        "            _, _, seg = model(imgs)\n",
        "            seg_H, seg_W = seg.shape[2:]\n",
        "            if masks.ndim == 3:\n",
        "                masks = masks.unsqueeze(1)\n",
        "            masks = masks.float()\n",
        "            masks = torch.nn.functional.interpolate(masks, size=(seg_H, seg_W), mode=\"nearest\")\n",
        "            masks = masks.squeeze(1).long()\n",
        "            masks = masks.clone()\n",
        "            masks[(masks > 20) & (masks != 255)] = 255\n",
        "\n",
        "            pred = torch.argmax(seg, dim=1)\n",
        "            preds.append(pred)\n",
        "            targets.append(masks)\n",
        "    preds = torch.cat(preds)\n",
        "    targets = torch.cat(targets)\n",
        "    return compute_mIoU(preds, targets)\n",
        "\n",
        "# Classification評估\n",
        "def evaluate_classification(model, val_loader, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    all_logits = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            logits, _, _ = model(imgs)\n",
        "            all_logits.append(logits.cpu())\n",
        "            all_labels.append(labels)\n",
        "    logits = torch.cat(all_logits)\n",
        "    labels = torch.cat(all_labels)\n",
        "    probs = F.softmax(logits, dim=1).numpy()\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "    top1 = (preds == labels.numpy()).mean()\n",
        "    try:\n",
        "        mAP = average_precision_score(np.eye(probs.shape[1])[labels.numpy()], probs, average='macro')\n",
        "    except:\n",
        "        mAP = 0.0\n",
        "    return mAP, top1\n"
      ],
      "metadata": {
        "id": "OAVoLaCCaWIO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Warmup ========== #\n",
        "def stage0_warmup(model, dummy_input_shape=(1, 3, 512, 512), device=\"cuda\"):\n",
        "    print(\"🔥 Stage 0: warm-up / ImageNet pretrain (可跳過，已加載權重)\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    x = torch.randn(dummy_input_shape).to(device)\n",
        "    with torch.no_grad():\n",
        "        _ = model(x)\n",
        "    print(\"✅ Warm-up 完成\")\n",
        "\n",
        "# ========== Segmentation 專訓 ========== #\n",
        "def stage1_segmentation(model, train_dir, val_dir, device=\"cuda\"):\n",
        "    print(\"🎯 Stage 1: segmentation 訓練中（只更新 seg_head）\")\n",
        "    mIoU_base = train_segmentation_stage1(model, train_dir, val_dir, device=device)\n",
        "    return mIoU_base\n",
        "\n",
        "# ========== Detection 專訓 ========== #\n",
        "def stage2_detection(model, train_loader, val_loader, mIoU_base, device=\"cuda\"):\n",
        "    print(\"🔎 Stage 2: detection-only 訓練中\")\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    model = model.to(device)\n",
        "    loss_fn = torch.nn.MSELoss()  # (簡化) 可用 L1+conf BCE\n",
        "    model.train()\n",
        "    for epoch in range(3):\n",
        "        for imgs, det_targets in train_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            det_targets = det_targets.to(device)\n",
        "            _, det_out, _ = model(imgs)\n",
        "            loss = loss_fn(det_out, det_targets)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch+1}/3 Done.\")\n",
        "    # Segmentation再評估一次\n",
        "    mIoU_after_det = evaluate_segmentation(model, val_loader, device=device)\n",
        "    mIoU_drop = max(0, mIoU_base - mIoU_after_det)\n",
        "    print(f\"📉 mIoU_drop: {mIoU_drop:.4f}\")\n",
        "    return mIoU_drop\n",
        "\n",
        "# ========== Classification 專訓 ========== #\n",
        "def stage3_classification(model, train_loader, val_loader, mIoU_base, mAP_base, Top1_base, device=\"cuda\"):\n",
        "    print(\"📘 Stage 3: classification-only 訓練中\")\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(3):\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            cls_logits, _, _ = model(imgs)\n",
        "            loss = criterion(cls_logits, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    # 評估 Seg / Det / Cls 效能\n",
        "    mIoU_now = evaluate_segmentation(model, val_loader, device=device)\n",
        "    mIoU_drop = max(0, mIoU_base - mIoU_now)\n",
        "    mAP_now, top1_now = evaluate_classification(model, val_loader, device=device)\n",
        "    mAP_drop = max(0, mAP_base - mAP_now)\n",
        "    top1_drop = max(0, Top1_base - top1_now)\n",
        "    print(f\"📉 mIoU_drop: {mIoU_drop:.4f}, mAP_drop: {mAP_drop:.4f}, Top1_drop: {top1_drop:.4f}\")\n",
        "    return mIoU_drop, mAP_drop, top1_drop\n"
      ],
      "metadata": {
        "id": "FYp7hgTlaYUu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 1: 初始化模型 ===\n",
        "model = UnifiedModel()\n",
        "\n",
        "# === Step 2: Stage 0 Warm-up（可選）===\n",
        "stage0_warmup(model)\n",
        "\n",
        "# === Step 3: Stage 1 segmentation（VOC）===\n",
        "mIoU_base = stage1_segmentation(model, train_dir=\"data/mini_voc_seg/train\", val_dir=\"data/mini_voc_seg/val\")\n",
        "\n",
        "# === Step 4: Stage 2 detection（COCO）===\n",
        "det_tf = transforms.Compose([transforms.Resize((512, 512)), transforms.ToTensor()])\n",
        "det_train = MiniCocoDetection(\"data/mini_coco_det/train\", \"data/mini_coco_det/annotations/mini_instances_val2017.json\", transform=det_tf)\n",
        "det_val = VOCSegmentationDataset(\"data/mini_voc_seg/val\", det_tf, None)\n",
        "mIoU_drop = stage2_detection(model, DataLoader(det_train, batch_size=8), DataLoader(det_val, batch_size=1), mIoU_base)\n",
        "\n",
        "# === Step 5: Stage 3 classification（Imagenette）===\n",
        "cls_train, cls_val = build_cls_dataset(\"data/imagenette_160\")\n",
        "mAP_base, Top1_base = evaluate_classification(model, DataLoader(cls_val, batch_size=8))\n",
        "mIoU_drop_3, mAP_drop, Top1_drop = stage3_classification(\n",
        "    model, DataLoader(cls_train, batch_size=8), DataLoader(cls_val, batch_size=8), mIoU_base, mAP_base, Top1_base\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4eROYbJsaapw",
        "outputId": "9b7e6231-e893-428b-eb3b-b6361ca32b07"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔥 Stage 0: warm-up / ImageNet pretrain (可跳過，已加載權重)\n",
            "✅ Warm-up 完成\n",
            "🎯 Stage 1: segmentation 訓練中（只更新 seg_head）\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 15/15 [00:03<00:00,  4.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Epoch 1 Loss: 1.6657\n",
            "📊 mIoU: 0.0804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 15/15 [00:03<00:00,  4.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Epoch 2 Loss: 0.6541\n",
            "📊 mIoU: 0.1903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 15/15 [00:02<00:00,  6.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Epoch 3 Loss: 0.3023\n",
            "📊 mIoU: 0.2378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 15/15 [00:02<00:00,  6.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Epoch 4 Loss: 0.1986\n",
            "📊 mIoU: 0.2240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 15/15 [00:02<00:00,  5.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Epoch 5 Loss: 0.1440\n",
            "📊 mIoU: 0.2320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 15/15 [00:03<00:00,  4.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Epoch 6 Loss: 0.0838\n",
            "📊 mIoU: 0.2432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 15/15 [00:02<00:00,  6.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Epoch 7 Loss: 0.0563\n",
            "📊 mIoU: 0.2560\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 15/15 [00:02<00:00,  6.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Epoch 8 Loss: 0.0391\n",
            "📊 mIoU: 0.2645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 15/15 [00:02<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Epoch 9 Loss: 0.0328\n",
            "📊 mIoU: 0.2712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 15/15 [00:02<00:00,  6.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Epoch 10 Loss: 0.0261\n",
            "📊 mIoU: 0.2735\n",
            "\n",
            "✅ Stage 1 完成！記錄 mIoU_base = 0.2735\n",
            "🔎 Stage 2: detection-only 訓練中\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "stack expects each tensor to be equal size, but got [19, 5] at entry 0 and [14, 5] at entry 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-77e6bfb92e3a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdet_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMiniCocoDetection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/mini_coco_det/train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/mini_coco_det/annotations/mini_instances_val2017.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdet_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdet_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVOCSegmentationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/mini_voc_seg/val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdet_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmIoU_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage2_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmIoU_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# === Step 5: Stage 3 classification（Imagenette）===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-ff4078aa40d0>\u001b[0m in \u001b[0;36mstage2_detection\u001b[0;34m(model, train_loader, val_loader, mIoU_base, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdet_targets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mdet_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdet_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             return [\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             ]  # Backwards compatibility.\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [19, 5] at entry 0 and [14, 5] at entry 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "\n",
        "img_dir = \"data/mini_coco_det/train\"\n",
        "json_path = \"data/mini_coco_det/annotations/instances_val2017.json\"\n",
        "mini_json_path = \"data/mini_coco_det/annotations/mini_instances_val2017.json\"\n",
        "\n",
        "with open(json_path) as f:\n",
        "    ann = json.load(f)\n",
        "img_files = {f for f in os.listdir(img_dir) if f.endswith('.jpg')}\n",
        "filtered_images = []\n",
        "valid_img_ids = set()\n",
        "\n",
        "for img in ann['images']:\n",
        "    if img['file_name'] in img_files:\n",
        "        filtered_images.append(img)\n",
        "        valid_img_ids.add(img['id'])\n",
        "\n",
        "filtered_anns = [a for a in ann['annotations'] if a['image_id'] in valid_img_ids]\n",
        "mini_ann = {\n",
        "    \"info\": ann.get('info', {}),\n",
        "    \"licenses\": ann.get('licenses', []),\n",
        "    \"categories\": ann['categories'],\n",
        "    \"images\": filtered_images,\n",
        "    \"annotations\": filtered_anns,\n",
        "}\n",
        "with open(mini_json_path, \"w\") as f:\n",
        "    json.dump(mini_ann, f)\n",
        "print(f\"已寫入 mini_instances_val2017.json，共有 {len(filtered_images)} 張圖\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWRujVbMexRR",
        "outputId": "a4b97048-6c39-4f58-9e83-ffe619f65ca5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "已寫入 mini_instances_val2017.json，共有 240 張圖\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# 路徑\n",
        "img_dir = Path(\"data/mini_coco_det/train\")\n",
        "json_path = \"data/mini_coco_det/annotations/mini_instances_val2017.json\"\n",
        "\n",
        "# 讀 JSON\n",
        "with open(json_path) as f:\n",
        "    coco = json.load(f)\n",
        "\n",
        "json_files = set(img['file_name'] for img in coco['images'])\n",
        "real_files = set(p.name for p in img_dir.glob(\"*.jpg\"))\n",
        "\n",
        "missing_files = json_files - real_files\n",
        "print(f\"🚨 JSON 中缺失的圖片：{len(missing_files)}\")\n",
        "for f in list(missing_files)[:10]:\n",
        "    print(\" -\", f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXAM_sHWGuQn",
        "outputId": "e7af5a12-6b54-4c2c-dbbe-e3f271deb7cc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚨 JSON 中缺失的圖片：0\n"
          ]
        }
      ]
    }
  ]
}