{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMXSanmeiA5gb2diWhlUPxa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kwannn666/DL_Report2/blob/main/DL_Report2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OzGnZ8dRm7BQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5659187a-71b6-476f-eee0-dc3b2d59729a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m962.5/962.5 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h--2025-06-10 08:48:10--  https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.107.86, 52.217.140.80, 54.231.133.88, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.107.86|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99003388 (94M) [application/x-tar]\n",
            "Saving to: â€˜imagenette2-160.tgzâ€™\n",
            "\n",
            "imagenette2-160.tgz 100%[===================>]  94.42M  29.0MB/s    in 3.3s    \n",
            "\n",
            "2025-06-10 08:48:14 (29.0 MB/s) - â€˜imagenette2-160.tgzâ€™ saved [99003388/99003388]\n",
            "\n",
            "--2025-06-10 08:48:16--  http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
            "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
            "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1999639040 (1.9G) [application/x-tar]\n",
            "Saving to: â€˜VOCtrainval_11-May-2012.tarâ€™\n",
            "\n",
            "VOCtrainval_11-May- 100%[===================>]   1.86G  54.2MB/s    in 29s     \n",
            "\n",
            "2025-06-10 08:48:45 (65.6 MB/s) - â€˜VOCtrainval_11-May-2012.tarâ€™ saved [1999639040/1999639040]\n",
            "\n",
            "--2025-06-10 08:48:59--  http://images.cocodataset.org/zips/val2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.165.41, 52.217.225.65, 16.15.193.72, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.165.41|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815585330 (778M) [application/zip]\n",
            "Saving to: â€˜val2017.zipâ€™\n",
            "\n",
            "val2017.zip         100%[===================>] 777.80M  37.8MB/s    in 21s     \n",
            "\n",
            "2025-06-10 08:49:20 (37.8 MB/s) - â€˜val2017.zipâ€™ saved [815585330/815585330]\n",
            "\n",
            "--2025-06-10 08:49:29--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.160.97, 52.216.44.169, 52.217.234.161, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.160.97|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: â€˜annotations_trainval2017.zipâ€™\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  41.8MB/s    in 8.4s    \n",
            "\n",
            "2025-06-10 08:49:37 (28.9 MB/s) - â€˜annotations_trainval2017.zipâ€™ saved [252907541/252907541]\n",
            "\n",
            "mv: cannot move 'data/tmp/annotations/instances_val2017.json' to 'data/mini_coco_det/annotations/': Not a directory\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“¦ å®‰è£å¥—ä»¶èˆ‡æ¸…ç†èˆŠè³‡æ–™\n",
        "!pip install -q torchvision lightning\n",
        "\n",
        "# âœ… æ¸…é™¤æ®˜ç•™è³‡æ–™ï¼ˆé¿å…è§£å£“æ™‚äº¤äº’å¼æç¤ºï¼‰\n",
        "!rm -rf data\n",
        "!rm -rf imagenette2-160.tgz VOCtrainval_11-May-2012.tar val2017.zip annotations_trainval2017.zip\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import json\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# å›ºå®šäº‚æ•¸ç¨®å­\n",
        "random.seed(42)\n",
        "\n",
        "# âœ… å»ºç«‹ä¸»çµæ§‹\n",
        "base_dir = \"data\"\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "for name in [\"mini_coco_det\", \"mini_voc_seg\", \"imagenette_160\"]:\n",
        "    for sub in [\"train\", \"val\"]:\n",
        "        os.makedirs(os.path.join(base_dir, name, sub), exist_ok=True)\n",
        "\n",
        "# âœ… ä¸‹è¼‰è³‡æ–™\n",
        "!wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\n",
        "!tar -xf imagenette2-160.tgz -C data/\n",
        "!mv data/imagenette2-160 data/imagenette_160_raw\n",
        "\n",
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
        "!tar -xf VOCtrainval_11-May-2012.tar -C data/\n",
        "\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip\n",
        "!unzip -q val2017.zip -d data/mini_coco_det/train\n",
        "\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "!unzip -q annotations_trainval2017.zip -d data/tmp\n",
        "!mv data/tmp/annotations/instances_val2017.json data/mini_coco_det/annotations/\n",
        "!rm -r data/tmp\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å»ºç«‹æ­£ç¢ºçš„ç›®éŒ„ï¼ˆå¦‚æœé‚„æ²’å»ºç«‹ï¼‰\n",
        "!mkdir -p data/mini_coco_det/annotations\n",
        "\n",
        "# å°‡æ¨™è¨»æª”æ¬åˆ°æ­£å¼ç›®éŒ„ä¸­\n",
        "!mv data/tmp/annotations/instances_val2017.json data/mini_coco_det/annotations/\n",
        "\n",
        "# åˆªé™¤æš«å­˜è³‡æ–™å¤¾\n",
        "!rm -r data/tmp\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQxTrD0d5wz4",
        "outputId": "b39c24f2-eeac-4b4f-8c73-1670af9388c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'data/tmp/annotations/instances_val2017.json': No such file or directory\n",
            "rm: cannot remove 'data/tmp': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# ğŸ”„ æ¸…é™¤èˆŠçš„ç›®æ¨™è³‡æ–™å¤¾ï¼ˆä¸åˆªåŸå§‹è§£å£“æª”ï¼‰\n",
        "# =============================\n",
        "for subdir in [\"data/imagenette_160\", \"data/mini_voc_seg\", \"data/mini_coco_det\"]:\n",
        "    if os.path.exists(subdir):\n",
        "        shutil.rmtree(subdir)\n",
        "    os.makedirs(subdir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "wJP3k24PGrum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q annotations_trainval2017.zip -d data/tmp\n"
      ],
      "metadata": {
        "id": "Uh3HiSF9Dlus"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data/tmp/annotations/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUWQX4OnDsL0",
        "outputId": "0d4d7b4e-c0a0-40b5-bc6b-517fb5074c8b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "captions_train2017.json   instances_val2017.json\n",
            "captions_val2017.json\t  person_keypoints_train2017.json\n",
            "instances_train2017.json  person_keypoints_val2017.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data/mini_coco_det/annotations\n",
        "!mv data/tmp/annotations/instances_val2017.json data/mini_coco_det/annotations/\n",
        "!rm -r data/tmp\n"
      ],
      "metadata": {
        "id": "uj_0HzAsDvwS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.path.exists(\"data/mini_coco_det/annotations/instances_val2017.json\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Wje9S-WDgJe",
        "outputId": "86c51f16-8450-495c-8e67-3b3cd677d08f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, random, json, glob\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "\n",
        "# =============================\n",
        "# 1ï¸âƒ£ Imagenette æŠ½æ¨£èˆ‡è¤‡è£½\n",
        "# =============================\n",
        "imagenette_src = Path(\"data/imagenette_160_raw/train\")\n",
        "imagenette_dst_train = Path(\"data/imagenette_160/train\")\n",
        "imagenette_dst_val = Path(\"data/imagenette_160/val\")\n",
        "imagenette_dst_train.mkdir(parents=True, exist_ok=True)\n",
        "imagenette_dst_val.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "all_imgs = list(imagenette_src.rglob(\"*.JPEG\"))\n",
        "print(f\"ğŸ“¦ æ‰¾åˆ° Imagenette åŸå§‹åœ–ç‰‡æ•¸é‡ï¼š{len(all_imgs)}\")\n",
        "if len(all_imgs) < 300:\n",
        "    raise ValueError(\"âŒ Imagenette åœ–ç‰‡æ•¸ä¸è¶³ 300 å¼µï¼Œè«‹ç¢ºèªæ˜¯å¦æ­£ç¢ºè§£å£“ imagenette2-160.tgz\")\n",
        "\n",
        "selected = random.sample(all_imgs, 300)\n",
        "train_imgs, val_imgs = selected[:240], selected[240:]\n",
        "\n",
        "for img in tqdm(train_imgs, desc=\"Imagenette Train\"):\n",
        "    shutil.copy(img, imagenette_dst_train / img.name)\n",
        "for img in tqdm(val_imgs, desc=\"Imagenette Val\"):\n",
        "    shutil.copy(img, imagenette_dst_val / img.name)\n",
        "\n",
        "# =============================\n",
        "# 2ï¸âƒ£ VOC Segmentation æŠ½æ¨£èˆ‡é®ç½©å°æ‡‰\n",
        "# =============================\n",
        "voc_img_dir = Path(\"data/VOCdevkit/VOC2012/JPEGImages\")\n",
        "voc_mask_dir = Path(\"data/VOCdevkit/VOC2012/SegmentationClass\")\n",
        "voc_dst_train = Path(\"data/mini_voc_seg/train\")\n",
        "voc_dst_val = Path(\"data/mini_voc_seg/val\")\n",
        "voc_dst_train.mkdir(parents=True, exist_ok=True)\n",
        "voc_dst_val.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "all_voc_images = list(voc_img_dir.glob(\"*.jpg\"))\n",
        "mask_names = {p.stem for p in voc_mask_dir.glob(\"*.png\")}\n",
        "valid_voc_images = [img for img in all_voc_images if img.stem in mask_names]\n",
        "\n",
        "print(f\"ğŸ“¦ å¯ç”¨ VOC åœ–ç‰‡ï¼ˆå«é®ç½©ï¼‰æ•¸é‡ï¼š{len(valid_voc_images)}\")\n",
        "if len(valid_voc_images) < 300:\n",
        "    raise ValueError(\"âŒ VOC å¯ç”¨åœ–ç‰‡æ•¸ä¸è¶³ 300 å¼µï¼Œè«‹ç¢ºèª VOCtrainval æ˜¯å¦æ­£ç¢ºè§£å£“\")\n",
        "\n",
        "selected_voc = random.sample(valid_voc_images, 300)\n",
        "voc_train = selected_voc[:240]\n",
        "voc_val = selected_voc[240:]\n",
        "\n",
        "def copy_voc(img_list, target_dir):\n",
        "    for img in tqdm(img_list, desc=f\"VOC â†’ {target_dir.name}\"):\n",
        "        mask = voc_mask_dir / (img.stem + \".png\")\n",
        "        if mask.exists():\n",
        "            shutil.copy(img, target_dir / img.name)\n",
        "            shutil.copy(mask, target_dir / mask.name)\n",
        "\n",
        "copy_voc(voc_train, voc_dst_train)\n",
        "copy_voc(voc_val, voc_dst_val)\n",
        "\n",
        "# =============================\n",
        "# 3ï¸âƒ£ COCO Detection æŠ½æ¨£èˆ‡æ¨™è¨»éæ¿¾\n",
        "# =============================\n",
        "coco_img_dir = Path(\"data/mini_coco_det/train/val2017\")\n",
        "coco_dst_train = Path(\"data/mini_coco_det/train\")\n",
        "coco_dst_val = Path(\"data/mini_coco_det/val\")\n",
        "coco_dst_train.mkdir(parents=True, exist_ok=True)\n",
        "coco_dst_val.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "all_coco_images = list(coco_img_dir.glob(\"*.jpg\"))\n",
        "print(f\"ğŸ“¦ æ‰¾åˆ° COCO val2017 åœ–ç‰‡æ•¸é‡ï¼š{len(all_coco_images)}\")\n",
        "if len(all_coco_images) < 300:\n",
        "    raise ValueError(\"âŒ COCO åœ–ç‰‡ä¸è¶³ 300 å¼µï¼Œè«‹ç¢ºèª val2017 æ˜¯å¦æ­£ç¢ºè§£å£“\")\n",
        "\n",
        "selected_coco = random.sample(all_coco_images, 300)\n",
        "coco_train, coco_val = selected_coco[:240], selected_coco[240:]\n",
        "\n",
        "for img in tqdm(coco_train, desc=\"COCO Train\"):\n",
        "    shutil.copy(img, coco_dst_train / img.name)\n",
        "for img in tqdm(coco_val, desc=\"COCO Val\"):\n",
        "    shutil.copy(img, coco_dst_val / img.name)\n",
        "\n",
        "# æŒ‡å®š COCO æ¨™è¨»æª”\n",
        "coco_ann_path = Path(\"data/mini_coco_det/annotations/instances_val2017.json\")\n",
        "if not coco_ann_path.exists():\n",
        "    raise FileNotFoundError(\"âŒ æ‰¾ä¸åˆ° COCO æ¨™è¨»æª” instances_val2017.jsonï¼Œè«‹ç¢ºèªæ˜¯å¦æ­£ç¢ºæ¬ç§»\")\n",
        "\n",
        "with open(coco_ann_path) as f:\n",
        "    coco_ann = json.load(f)\n",
        "\n",
        "valid_img_names = {p.name for p in selected_coco}\n",
        "valid_img_ids = set()\n",
        "filtered_images = []\n",
        "for img in coco_ann[\"images\"]:\n",
        "    if img[\"file_name\"] in valid_img_names:\n",
        "        filtered_images.append(img)\n",
        "        valid_img_ids.add(img[\"id\"])\n",
        "\n",
        "filtered_anns = [ann for ann in coco_ann[\"annotations\"] if ann[\"image_id\"] in valid_img_ids]\n",
        "filtered_coco = {\n",
        "    \"info\": coco_ann.get(\"info\", {}),\n",
        "    \"licenses\": coco_ann.get(\"licenses\", []),\n",
        "    \"categories\": coco_ann[\"categories\"],\n",
        "    \"images\": filtered_images,\n",
        "    \"annotations\": filtered_anns\n",
        "}\n",
        "\n",
        "# å„²å­˜éæ¿¾å¾Œçš„æ¨™è¨» JSON\n",
        "mini_json_path = Path(\"data/mini_coco_det/annotations/instances_val2017.json\")\n",
        "with open(mini_json_path, \"w\") as f:\n",
        "    json.dump(filtered_coco, f)\n",
        "\n",
        "# ç§»é™¤åŸå§‹ val2017\n",
        "shutil.rmtree(coco_img_dir)\n",
        "\n",
        "print(\"\\nâœ… æ‰€æœ‰ Mini Dataset æº–å‚™å®Œæˆï¼ä¸‰çµ„è³‡æ–™å„åŒ…å« train 240 å¼µã€val 60 å¼µã€‚\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqNQUhUu5hnr",
        "outputId": "ed1cac7f-c375-4e13-c764-6e2cc120891a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ æ‰¾åˆ° Imagenette åŸå§‹åœ–ç‰‡æ•¸é‡ï¼š9469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Imagenette Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:00<00:00, 6056.25it/s]\n",
            "Imagenette Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:00<00:00, 5459.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ å¯ç”¨ VOC åœ–ç‰‡ï¼ˆå«é®ç½©ï¼‰æ•¸é‡ï¼š2913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "VOC â†’ train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:00<00:00, 954.48it/s]\n",
            "VOC â†’ val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:00<00:00, 889.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ æ‰¾åˆ° COCO val2017 åœ–ç‰‡æ•¸é‡ï¼š5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "COCO Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:00<00:00, 2719.51it/s]\n",
            "COCO Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:00<00:00, 2454.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… æ‰€æœ‰ Mini Dataset æº–å‚™å®Œæˆï¼ä¸‰çµ„è³‡æ–™å„åŒ…å« train 240 å¼µã€val 60 å¼µã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”„ æ¸…ç©ºåŸæœ‰ VOC è¼¸å‡ºè³‡æ–™å¤¾\n",
        "!rm -rf data/mini_voc_seg/train/*\n",
        "!rm -rf data/mini_voc_seg/val/*\n",
        "\n",
        "# ğŸŸ© é‡å»º VOC train/val\n",
        "voc_img_dir = Path(\"data/VOCdevkit/VOC2012/JPEGImages\")\n",
        "voc_mask_dir = Path(\"data/VOCdevkit/VOC2012/SegmentationClass\")\n",
        "voc_dst_train = Path(\"data/mini_voc_seg/train\")\n",
        "voc_dst_val = Path(\"data/mini_voc_seg/val\")\n",
        "voc_dst_train.mkdir(parents=True, exist_ok=True)\n",
        "voc_dst_val.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "all_voc_images = list(voc_img_dir.glob(\"*.jpg\"))\n",
        "mask_names = {p.stem for p in voc_mask_dir.glob(\"*.png\")}\n",
        "valid_voc_images = [img for img in all_voc_images if img.stem in mask_names]\n",
        "\n",
        "print(f\"ğŸ“¦ å¯ç”¨ VOC åœ–ç‰‡ï¼ˆå«é®ç½©ï¼‰æ•¸é‡ï¼š{len(valid_voc_images)}\")\n",
        "selected_voc = random.sample(valid_voc_images, 150)\n",
        "voc_train = selected_voc[:120]\n",
        "voc_val = selected_voc[120:]\n",
        "\n",
        "def copy_voc(img_list, target_dir):\n",
        "    for img in tqdm(img_list, desc=f\"VOC â†’ {target_dir.name}\"):\n",
        "        mask = voc_mask_dir / (img.stem + \".png\")\n",
        "        if mask.exists():\n",
        "            shutil.copy(img, target_dir / img.name)\n",
        "            shutil.copy(mask, target_dir / mask.name)\n",
        "\n",
        "copy_voc(voc_train, voc_dst_train)\n",
        "copy_voc(voc_val, voc_dst_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9C1K_3xIF2h",
        "outputId": "958ea99e-f777-432c-9d38-f78d007df0c7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ å¯ç”¨ VOC åœ–ç‰‡ï¼ˆå«é®ç½©ï¼‰æ•¸é‡ï¼š2913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "VOC â†’ train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:00<00:00, 1407.40it/s]\n",
            "VOC â†’ val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 588.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os\n",
        "\n",
        "# ğŸ”„ åˆªé™¤æ•´å€‹ train/val è³‡æ–™å¤¾\n",
        "shutil.rmtree(\"data/mini_voc_seg/train\", ignore_errors=True)\n",
        "shutil.rmtree(\"data/mini_voc_seg/val\", ignore_errors=True)\n",
        "\n",
        "# âœ… é‡æ–°å»ºç«‹è³‡æ–™å¤¾\n",
        "os.makedirs(\"data/mini_voc_seg/train\", exist_ok=True)\n",
        "os.makedirs(\"data/mini_voc_seg/val\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "eQzA4EL5Iu4U"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def count_images(folder):\n",
        "    return len(list(Path(folder).glob(\"*.jpg\"))) + len(list(Path(folder).glob(\"*.jpeg\"))) + len(list(Path(folder).glob(\"*.png\")))+ len(list(Path(folder).glob(\"*.JPEG\")))\n",
        "\n",
        "print(\"COCO Train:\", count_images(\"data/mini_coco_det/train\"))\n",
        "print(\"COCO Val:\", count_images(\"data/mini_coco_det/val\"))\n",
        "\n",
        "print(\"VOC Train:\", count_images(\"data/mini_voc_seg/train\"))\n",
        "print(\"VOC Val:\", count_images(\"data/mini_voc_seg/val\"))\n",
        "\n",
        "print(\"Imagenette Train:\", count_images(\"data/imagenette_160/train\"))\n",
        "print(\"Imagenette Val:\", count_images(\"data/imagenette_160/val\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KoQQbag7mVl",
        "outputId": "ae5b1bbd-ccf4-4914-e6b0-53effeb309ec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COCO Train: 240\n",
            "COCO Val: 60\n",
            "VOC Train: 240\n",
            "VOC Val: 60\n",
            "Imagenette Train: 240\n",
            "Imagenette Val: 60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "i6BHnumt8xJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#é¡¯ç¤ºä¸‰å€‹ä»»å‹™è³‡æ–™å¤¾çš„ç¸½æª”æ¡ˆå¤§å°\n",
        "def get_folder_size_mb(path):\n",
        "    total_size = 0\n",
        "    for dirpath, _, filenames in os.walk(path):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            if os.path.isfile(fp):\n",
        "                total_size += os.path.getsize(fp)\n",
        "    return round(total_size / (1024 * 1024), 2)  # è½‰æˆ MB ä¸¦å››æ¨äº”å…¥\n",
        "\n",
        "# ä¸‰å€‹è³‡æ–™å¤¾è·¯å¾‘\n",
        "folders = {\n",
        "    \"mini_coco_det\": \"data/mini_coco_det\",\n",
        "    \"mini_voc_seg\": \"data/mini_voc_seg\",\n",
        "    \"imagenette_160\": \"data/imagenette_160\"\n",
        "}\n",
        "\n",
        "# å°å‡ºå¤§å°\n",
        "print(\"ğŸ“¦ å„è³‡æ–™å¤¾ç¸½æª”æ¡ˆå¤§å°ï¼ˆMBï¼‰\")\n",
        "for name, path in folders.items():\n",
        "    size_mb = get_folder_size_mb(path)\n",
        "    print(f\"{name.ljust(20)}: {size_mb} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "valr1UuX_eSJ",
        "outputId": "c23306fe-a246-460c-8a79-f8db36aa9996"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ å„è³‡æ–™å¤¾ç¸½æª”æ¡ˆå¤§å°ï¼ˆMBï¼‰\n",
            "mini_coco_det       : 48.53 MB\n",
            "mini_voc_seg        : 17.08 MB\n",
            "imagenette_160      : 2.27 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q thop"
      ],
      "metadata": {
        "id": "UhH9CWvGMBK8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork, LastLevelMaxPool\n",
        "from thop import profile\n",
        "import time"
      ],
      "metadata": {
        "id": "ZNEOrVQEL2os"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ§  çµ±ä¸€æ¨¡å‹å®šç¾©\n",
        "class UnifiedModel(nn.Module):\n",
        "    def __init__(self, num_classes_cls=10, num_classes_det=10, num_classes_seg=21):\n",
        "        super(UnifiedModel, self).__init__()\n",
        "\n",
        "        # ğŸ”— Backbone: MobileNetV3 Small\n",
        "        backbone = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
        "        self.backbone = backbone.features\n",
        "\n",
        "        # ğŸ”— FPN Neckï¼šæ¥ 3 å±¤è¼¸å‡º (channel=24, 48, 96)\n",
        "        self.fpn = FeaturePyramidNetwork(\n",
        "           in_channels_list=[24, 40, 576],  # â† ä¿®æ­£é€™è£¡\n",
        "           out_channels=128,\n",
        "           extra_blocks=LastLevelMaxPool()\n",
        "        )\n",
        "\n",
        "\n",
        "        # ğŸ§  Shared conv head\n",
        "        self.shared_head = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # ğŸ¯ Output heads\n",
        "        self.classifier = nn.Linear(128, num_classes_cls)         # åˆ†é¡\n",
        "        self.det_head = nn.Conv2d(128, num_classes_det * 5, 1)    # åµæ¸¬ (class + bbox)\n",
        "        self.seg_head = nn.Conv2d(128, num_classes_seg, 1)        # åˆ†å‰²\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = []\n",
        "        for i, layer in enumerate(self.backbone):\n",
        "            x = layer(x)\n",
        "            if i in [3, 6, 12]:\n",
        "                feats.append(x)\n",
        "\n",
        "        fpn_feats = self.fpn({str(i): f for i, f in enumerate(feats)})\n",
        "        fpn_out = list(fpn_feats.values())[0]\n",
        "\n",
        "        shared = self.shared_head(fpn_out)\n",
        "        B, C, H, W = shared.shape\n",
        "\n",
        "        cls_logits = self.classifier(torch.mean(shared.view(B, C, -1), dim=2))\n",
        "        det_raw = self.det_head(shared)\n",
        "        det_output = det_raw.permute(0, 2, 3, 1).reshape(B, -1, 5)\n",
        "        seg_mask = self.seg_head(shared)\n",
        "\n",
        "        return cls_logits, det_output, seg_mask\n",
        "\n",
        "# ğŸ“¦ åƒæ•¸æ•¸é‡çµ±è¨ˆ\n",
        "def count_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# â±ï¸ æ¨è«–æ™‚é–“æ¸¬é‡ï¼ˆmsï¼‰\n",
        "def measure_inference_time(model, input_shape=(1, 3, 512, 512), device=\"cpu\"):\n",
        "    x = torch.randn(input_shape).to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(5):  # warm-up\n",
        "            _ = model(x)\n",
        "        start = time.time()\n",
        "        for _ in range(10):\n",
        "            _ = model(x)\n",
        "        end = time.time()\n",
        "    return (end - start) / 10 * 1000  # å–®ä½: ms\n",
        "\n",
        "# ğŸš€ å»ºç«‹èˆ‡æ¸¬è©¦\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = UnifiedModel().to(device)\n",
        "params = count_params(model)\n",
        "inference_ms = measure_inference_time(model, device=device)\n",
        "\n",
        "# âœ… é¡¯ç¤ºçµæœ\n",
        "print(f\"âœ… æ¨¡å‹åƒæ•¸æ•¸é‡ï¼š{params:,} å€‹\")\n",
        "print(f\"âœ… å–®å¼µæ¨è«–æ™‚é–“ï¼š{inference_ms:.2f} ms\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "susIp5NfL7yo",
        "outputId": "c123e33b-0c31-4230-e3e3-08d58e6b5033"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.83M/9.83M [00:00<00:00, 78.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… æ¨¡å‹åƒæ•¸æ•¸é‡ï¼š1,758,193 å€‹\n",
            "âœ… å–®å¼µæ¨è«–æ™‚é–“ï¼š29.23 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "\n",
        "# ----------- Segmentation (VOC) Dataset -----------\n",
        "class VOCSegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None, target_transform=None):\n",
        "        self.image_paths = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg')])\n",
        "        self.mask_paths = [p.replace('.jpg', '.png') for p in self.image_paths]\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        mask = Image.open(self.mask_paths[idx]).convert('L')  # segmentation mask\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        if self.target_transform:\n",
        "            mask = self.target_transform(mask)\n",
        "        mask = mask.long()\n",
        "        # --- é˜²å‘†ï¼šåªå…è¨± 0~20/255 ---\n",
        "        mask = mask.clone()\n",
        "        mask[(mask > 20) & (mask != 255)] = 255\n",
        "        return img, mask\n",
        "\n",
        "# ----------- Detection (COCO mini) Dataset -----------\n",
        "class MiniCocoDetection(Dataset):\n",
        "    def __init__(self, img_dir, ann_path, transform=None):\n",
        "        self.img_dir = Path(img_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "        with open(ann_path) as f:\n",
        "            coco = json.load(f)\n",
        "\n",
        "        self.img_id_to_filename = {img['id']: img['file_name'] for img in coco['images']}\n",
        "        self.ann_by_img = {img_id: [] for img_id in self.img_id_to_filename}\n",
        "        for ann in coco['annotations']:\n",
        "            self.ann_by_img[ann['image_id']].append(ann)\n",
        "\n",
        "        self.ids = list(self.img_id_to_filename.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.ids[idx]\n",
        "        file_name = self.img_id_to_filename[img_id]\n",
        "        img_path = self.img_dir / file_name\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        anns = self.ann_by_img[img_id]\n",
        "\n",
        "        boxes = []\n",
        "        for ann in anns:\n",
        "            x, y, w, h = ann['bbox']\n",
        "            boxes.append([x, y, x + w, y + h])\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        # ç›®æ¨™æ ¼å¼ç‚º (N, 5): cx, cy, w, h, 1\n",
        "        targets = []\n",
        "        for box in boxes:\n",
        "            cx = (box[0] + box[2]) / 2\n",
        "            cy = (box[1] + box[3]) / 2\n",
        "            w = box[2] - box[0]\n",
        "            h = box[3] - box[1]\n",
        "            targets.append([cx, cy, w, h, 1])  # 1 for conf\n",
        "        targets = torch.tensor(targets, dtype=torch.float32)\n",
        "        return img, targets\n",
        "\n",
        "# ----------- Classification (Imagenette) Dataset -----------\n",
        "def build_cls_dataset(root_dir, input_size=512):\n",
        "    transform = transforms.Compose([transforms.Resize((input_size, input_size)), transforms.ToTensor()])\n",
        "    train_set = ImageFolder(root=Path(root_dir) / \"train\", transform=transform)\n",
        "    val_set = ImageFolder(root=Path(root_dir) / \"val\", transform=transform)\n",
        "    return train_set, val_set\n"
      ],
      "metadata": {
        "id": "E6kg-jRVaUEr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "det_train = MiniCocoDetection(\n",
        "    \"data/mini_coco_det/train\",\n",
        "    \"data/mini_coco_det/annotations/mini_instances_val2017.json\",\n",
        "    transform=det_tf\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "UPmGhkSAfiXw",
        "outputId": "d3a79be2-4768-475a-8764-3120dab6c566"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'det_tf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-366c8b71cf6f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"data/mini_coco_det/train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"data/mini_coco_det/annotations/mini_instances_val2017.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdet_tf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'det_tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "# mIoU è¨ˆç®—\n",
        "def compute_mIoU(preds, targets, num_classes=21):\n",
        "    preds = preds.cpu().numpy()\n",
        "    targets = targets.cpu().numpy()\n",
        "    ious = []\n",
        "    for cls in range(num_classes):\n",
        "        pred_inds = preds == cls\n",
        "        target_inds = targets == cls\n",
        "        intersection = (pred_inds & target_inds).sum()\n",
        "        union = (pred_inds | target_inds).sum()\n",
        "        if union == 0:\n",
        "            continue\n",
        "        ious.append(intersection / union)\n",
        "    return np.mean(ious) if ious else 0\n",
        "\n",
        "# Segmentation è¨“ç·´\n",
        "def train_segmentation_stage1(model, train_dir, val_dir, epochs=10, batch_size=8, lr=1e-3, device=\"cuda\"):\n",
        "    input_size = 512\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((input_size, input_size)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    mask_transform = transforms.Compose([\n",
        "        transforms.Resize((input_size, input_size), interpolation=Image.NEAREST),\n",
        "        transforms.PILToTensor()\n",
        "    ])\n",
        "\n",
        "    train_set = VOCSegmentationDataset(train_dir, transform, mask_transform)\n",
        "    val_set = VOCSegmentationDataset(val_dir, transform, mask_transform)\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=1)\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
        "    best_mIoU = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        for imgs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            imgs, masks = imgs.to(device), masks.to(device)\n",
        "            _, _, seg = model(imgs)\n",
        "            seg_H, seg_W = seg.shape[2:]\n",
        "\n",
        "            # === å¼·åˆ¶ä¿®æ­£ mask çš„å€¼ç¯„åœ ===\n",
        "            if masks.ndim == 3:\n",
        "                masks = masks.unsqueeze(1)\n",
        "            masks = masks.float()\n",
        "            masks = torch.nn.functional.interpolate(masks, size=(seg_H, seg_W), mode=\"nearest\")\n",
        "            masks = masks.squeeze(1).long()\n",
        "            masks = masks.clone()\n",
        "            masks[(masks > 20) & (masks != 255)] = 255\n",
        "\n",
        "            loss = criterion(seg, masks)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"ğŸ”§ Epoch {epoch+1} Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        all_preds, all_targets = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, masks in val_loader:\n",
        "                imgs, masks = imgs.to(device), masks.to(device)\n",
        "                _, _, seg = model(imgs)\n",
        "                seg_H, seg_W = seg.shape[2:]\n",
        "                if masks.ndim == 3:\n",
        "                    masks = masks.unsqueeze(1)\n",
        "                masks = masks.float()\n",
        "                masks = torch.nn.functional.interpolate(masks, size=(seg_H, seg_W), mode=\"nearest\")\n",
        "                masks = masks.squeeze(1).long()\n",
        "                masks = masks.clone()\n",
        "                masks[(masks > 20) & (masks != 255)] = 255\n",
        "\n",
        "                preds = torch.argmax(seg, dim=1)\n",
        "                all_preds.append(preds)\n",
        "                all_targets.append(masks)\n",
        "        all_preds = torch.cat(all_preds)\n",
        "        all_targets = torch.cat(all_targets)\n",
        "        mIoU = compute_mIoU(all_preds, all_targets)\n",
        "        best_mIoU = max(best_mIoU, mIoU)\n",
        "        print(f\"ğŸ“Š mIoU: {mIoU:.4f}\")\n",
        "    print(f\"\\nâœ… Stage 1 å®Œæˆï¼è¨˜éŒ„ mIoU_base = {best_mIoU:.4f}\")\n",
        "    return best_mIoU\n",
        "\n",
        "# mIoU for segmentationå¿«é€Ÿè©•ä¼°\n",
        "def evaluate_segmentation(model, val_loader, device=\"cuda\", num_samples=20):\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for i, (imgs, masks) in enumerate(val_loader):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "            imgs, masks = imgs.to(device), masks.to(device)\n",
        "            _, _, seg = model(imgs)\n",
        "            seg_H, seg_W = seg.shape[2:]\n",
        "            if masks.ndim == 3:\n",
        "                masks = masks.unsqueeze(1)\n",
        "            masks = masks.float()\n",
        "            masks = torch.nn.functional.interpolate(masks, size=(seg_H, seg_W), mode=\"nearest\")\n",
        "            masks = masks.squeeze(1).long()\n",
        "            masks = masks.clone()\n",
        "            masks[(masks > 20) & (masks != 255)] = 255\n",
        "\n",
        "            pred = torch.argmax(seg, dim=1)\n",
        "            preds.append(pred)\n",
        "            targets.append(masks)\n",
        "    preds = torch.cat(preds)\n",
        "    targets = torch.cat(targets)\n",
        "    return compute_mIoU(preds, targets)\n",
        "\n",
        "# Classificationè©•ä¼°\n",
        "def evaluate_classification(model, val_loader, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    all_logits = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            logits, _, _ = model(imgs)\n",
        "            all_logits.append(logits.cpu())\n",
        "            all_labels.append(labels)\n",
        "    logits = torch.cat(all_logits)\n",
        "    labels = torch.cat(all_labels)\n",
        "    probs = F.softmax(logits, dim=1).numpy()\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "    top1 = (preds == labels.numpy()).mean()\n",
        "    try:\n",
        "        mAP = average_precision_score(np.eye(probs.shape[1])[labels.numpy()], probs, average='macro')\n",
        "    except:\n",
        "        mAP = 0.0\n",
        "    return mAP, top1\n"
      ],
      "metadata": {
        "id": "OAVoLaCCaWIO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Warmup ========== #\n",
        "def stage0_warmup(model, dummy_input_shape=(1, 3, 512, 512), device=\"cuda\"):\n",
        "    print(\"ğŸ”¥ Stage 0: warm-up / ImageNet pretrain (å¯è·³éï¼Œå·²åŠ è¼‰æ¬Šé‡)\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    x = torch.randn(dummy_input_shape).to(device)\n",
        "    with torch.no_grad():\n",
        "        _ = model(x)\n",
        "    print(\"âœ… Warm-up å®Œæˆ\")\n",
        "\n",
        "# ========== Segmentation å°ˆè¨“ ========== #\n",
        "def stage1_segmentation(model, train_dir, val_dir, device=\"cuda\"):\n",
        "    print(\"ğŸ¯ Stage 1: segmentation è¨“ç·´ä¸­ï¼ˆåªæ›´æ–° seg_headï¼‰\")\n",
        "    mIoU_base = train_segmentation_stage1(model, train_dir, val_dir, device=device)\n",
        "    return mIoU_base\n",
        "\n",
        "# ========== Detection å°ˆè¨“ ========== #\n",
        "def stage2_detection(model, train_loader, val_loader, mIoU_base, device=\"cuda\"):\n",
        "    print(\"ğŸ” Stage 2: detection-only è¨“ç·´ä¸­\")\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    model = model.to(device)\n",
        "    loss_fn = torch.nn.MSELoss()  # (ç°¡åŒ–) å¯ç”¨ L1+conf BCE\n",
        "    model.train()\n",
        "    for epoch in range(3):\n",
        "        for imgs, det_targets in train_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            det_targets = det_targets.to(device)\n",
        "            _, det_out, _ = model(imgs)\n",
        "            loss = loss_fn(det_out, det_targets)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch+1}/3 Done.\")\n",
        "    # Segmentationå†è©•ä¼°ä¸€æ¬¡\n",
        "    mIoU_after_det = evaluate_segmentation(model, val_loader, device=device)\n",
        "    mIoU_drop = max(0, mIoU_base - mIoU_after_det)\n",
        "    print(f\"ğŸ“‰ mIoU_drop: {mIoU_drop:.4f}\")\n",
        "    return mIoU_drop\n",
        "\n",
        "# ========== Classification å°ˆè¨“ ========== #\n",
        "def stage3_classification(model, train_loader, val_loader, mIoU_base, mAP_base, Top1_base, device=\"cuda\"):\n",
        "    print(\"ğŸ“˜ Stage 3: classification-only è¨“ç·´ä¸­\")\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(3):\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            cls_logits, _, _ = model(imgs)\n",
        "            loss = criterion(cls_logits, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    # è©•ä¼° Seg / Det / Cls æ•ˆèƒ½\n",
        "    mIoU_now = evaluate_segmentation(model, val_loader, device=device)\n",
        "    mIoU_drop = max(0, mIoU_base - mIoU_now)\n",
        "    mAP_now, top1_now = evaluate_classification(model, val_loader, device=device)\n",
        "    mAP_drop = max(0, mAP_base - mAP_now)\n",
        "    top1_drop = max(0, Top1_base - top1_now)\n",
        "    print(f\"ğŸ“‰ mIoU_drop: {mIoU_drop:.4f}, mAP_drop: {mAP_drop:.4f}, Top1_drop: {top1_drop:.4f}\")\n",
        "    return mIoU_drop, mAP_drop, top1_drop\n"
      ],
      "metadata": {
        "id": "FYp7hgTlaYUu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 1: åˆå§‹åŒ–æ¨¡å‹ ===\n",
        "model = UnifiedModel()\n",
        "\n",
        "# === Step 2: Stage 0 Warm-upï¼ˆå¯é¸ï¼‰===\n",
        "stage0_warmup(model)\n",
        "\n",
        "# === Step 3: Stage 1 segmentationï¼ˆVOCï¼‰===\n",
        "mIoU_base = stage1_segmentation(model, train_dir=\"data/mini_voc_seg/train\", val_dir=\"data/mini_voc_seg/val\")\n",
        "\n",
        "# === Step 4: Stage 2 detectionï¼ˆCOCOï¼‰===\n",
        "det_tf = transforms.Compose([transforms.Resize((512, 512)), transforms.ToTensor()])\n",
        "det_train = MiniCocoDetection(\"data/mini_coco_det/train\", \"data/mini_coco_det/annotations/mini_instances_val2017.json\", transform=det_tf)\n",
        "det_val = VOCSegmentationDataset(\"data/mini_voc_seg/val\", det_tf, None)\n",
        "mIoU_drop = stage2_detection(model, DataLoader(det_train, batch_size=8), DataLoader(det_val, batch_size=1), mIoU_base)\n",
        "\n",
        "# === Step 5: Stage 3 classificationï¼ˆImagenetteï¼‰===\n",
        "cls_train, cls_val = build_cls_dataset(\"data/imagenette_160\")\n",
        "mAP_base, Top1_base = evaluate_classification(model, DataLoader(cls_val, batch_size=8))\n",
        "mIoU_drop_3, mAP_drop, Top1_drop = stage3_classification(\n",
        "    model, DataLoader(cls_train, batch_size=8), DataLoader(cls_val, batch_size=8), mIoU_base, mAP_base, Top1_base\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4eROYbJsaapw",
        "outputId": "9b7e6231-e893-428b-eb3b-b6361ca32b07"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¥ Stage 0: warm-up / ImageNet pretrain (å¯è·³éï¼Œå·²åŠ è¼‰æ¬Šé‡)\n",
            "âœ… Warm-up å®Œæˆ\n",
            "ğŸ¯ Stage 1: segmentation è¨“ç·´ä¸­ï¼ˆåªæ›´æ–° seg_headï¼‰\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:03<00:00,  4.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Epoch 1 Loss: 1.6657\n",
            "ğŸ“Š mIoU: 0.0804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:03<00:00,  4.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Epoch 2 Loss: 0.6541\n",
            "ğŸ“Š mIoU: 0.1903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  6.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Epoch 3 Loss: 0.3023\n",
            "ğŸ“Š mIoU: 0.2378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  6.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Epoch 4 Loss: 0.1986\n",
            "ğŸ“Š mIoU: 0.2240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  5.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Epoch 5 Loss: 0.1440\n",
            "ğŸ“Š mIoU: 0.2320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:03<00:00,  4.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Epoch 6 Loss: 0.0838\n",
            "ğŸ“Š mIoU: 0.2432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  6.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Epoch 7 Loss: 0.0563\n",
            "ğŸ“Š mIoU: 0.2560\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  6.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Epoch 8 Loss: 0.0391\n",
            "ğŸ“Š mIoU: 0.2645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Epoch 9 Loss: 0.0328\n",
            "ğŸ“Š mIoU: 0.2712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  6.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Epoch 10 Loss: 0.0261\n",
            "ğŸ“Š mIoU: 0.2735\n",
            "\n",
            "âœ… Stage 1 å®Œæˆï¼è¨˜éŒ„ mIoU_base = 0.2735\n",
            "ğŸ” Stage 2: detection-only è¨“ç·´ä¸­\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "stack expects each tensor to be equal size, but got [19, 5] at entry 0 and [14, 5] at entry 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-77e6bfb92e3a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdet_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMiniCocoDetection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/mini_coco_det/train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/mini_coco_det/annotations/mini_instances_val2017.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdet_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdet_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVOCSegmentationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/mini_voc_seg/val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdet_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmIoU_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage2_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmIoU_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# === Step 5: Stage 3 classificationï¼ˆImagenetteï¼‰===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-ff4078aa40d0>\u001b[0m in \u001b[0;36mstage2_detection\u001b[0;34m(model, train_loader, val_loader, mIoU_base, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdet_targets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mdet_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdet_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             return [\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             ]  # Backwards compatibility.\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [19, 5] at entry 0 and [14, 5] at entry 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "\n",
        "img_dir = \"data/mini_coco_det/train\"\n",
        "json_path = \"data/mini_coco_det/annotations/instances_val2017.json\"\n",
        "mini_json_path = \"data/mini_coco_det/annotations/mini_instances_val2017.json\"\n",
        "\n",
        "with open(json_path) as f:\n",
        "    ann = json.load(f)\n",
        "img_files = {f for f in os.listdir(img_dir) if f.endswith('.jpg')}\n",
        "filtered_images = []\n",
        "valid_img_ids = set()\n",
        "\n",
        "for img in ann['images']:\n",
        "    if img['file_name'] in img_files:\n",
        "        filtered_images.append(img)\n",
        "        valid_img_ids.add(img['id'])\n",
        "\n",
        "filtered_anns = [a for a in ann['annotations'] if a['image_id'] in valid_img_ids]\n",
        "mini_ann = {\n",
        "    \"info\": ann.get('info', {}),\n",
        "    \"licenses\": ann.get('licenses', []),\n",
        "    \"categories\": ann['categories'],\n",
        "    \"images\": filtered_images,\n",
        "    \"annotations\": filtered_anns,\n",
        "}\n",
        "with open(mini_json_path, \"w\") as f:\n",
        "    json.dump(mini_ann, f)\n",
        "print(f\"å·²å¯«å…¥ mini_instances_val2017.jsonï¼Œå…±æœ‰ {len(filtered_images)} å¼µåœ–\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWRujVbMexRR",
        "outputId": "a4b97048-6c39-4f58-9e83-ffe619f65ca5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "å·²å¯«å…¥ mini_instances_val2017.jsonï¼Œå…±æœ‰ 240 å¼µåœ–\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# è·¯å¾‘\n",
        "img_dir = Path(\"data/mini_coco_det/train\")\n",
        "json_path = \"data/mini_coco_det/annotations/mini_instances_val2017.json\"\n",
        "\n",
        "# è®€ JSON\n",
        "with open(json_path) as f:\n",
        "    coco = json.load(f)\n",
        "\n",
        "json_files = set(img['file_name'] for img in coco['images'])\n",
        "real_files = set(p.name for p in img_dir.glob(\"*.jpg\"))\n",
        "\n",
        "missing_files = json_files - real_files\n",
        "print(f\"ğŸš¨ JSON ä¸­ç¼ºå¤±çš„åœ–ç‰‡ï¼š{len(missing_files)}\")\n",
        "for f in list(missing_files)[:10]:\n",
        "    print(\" -\", f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXAM_sHWGuQn",
        "outputId": "e7af5a12-6b54-4c2c-dbbe-e3f271deb7cc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš¨ JSON ä¸­ç¼ºå¤±çš„åœ–ç‰‡ï¼š0\n"
          ]
        }
      ]
    }
  ]
}